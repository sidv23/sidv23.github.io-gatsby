{"expireTime":9007200881984254000,"key":"transformer-remark-markdown-html-6810404930e1d00af47b13f36c67f784-gatsby-remark-relative-imagesgatsby-remark-katexgatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-autolink-headersgatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-external-links-","val":"<p><img src=\"/media/stag.png\"></p>\n<p>The Statistical Learning Theory working group will meet on Wednesdays. We will have readings, presentations and discussions on topics including (but, not limited to) statistical learning theory, nonparametric estimation and inference, deep learning, functional data analysis, topological data analysis and algebraic statistics. The focus of the group is to read and discuss important papers in one particular topic of interest for a semester or two.</p>\n<table>\n<thead>\n<tr>\n<th><span style=\"color: red\"> Time </span></th>\n<th>Wednesday 2:15 - 3:45 PM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span style=\"color: red\"> <strong>Location</strong> </span></td>\n<td><a href=\"https://psu.zoom.us/j/8996273242?pwd=VVo5UGhydE0wZFQxRnZWcGZsbXhZUT09\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Zoom</a></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"topics\" style=\"position:relative;\"><a href=\"#topics\" aria-label=\"topics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Topics</h2>\n<p>This semester we will focus on stochastic optimization. The papers we will focus on are categorized below.</p>\n<ol>\n<li>\n<p>Sampling and Gradient Flow:</p>\n<ul>\n<li><a href=\"https://francisbach.com/gradient-flows/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Francis Bach’s tutorial on gradient flows</a></li>\n<li><a href=\"https://www-dimat.unipv.it/savare/Ravello2010/JKO.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">The Variational Formulation of the Fokker-Planck Equation</a></li>\n<li><a href=\"https://arxiv.org/pdf/1705.09048.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Convergence of Langevin MCMC in\nKL-divergence</a></li>\n<li><a href=\"https://arxiv.org/pdf/1802.08089.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Sampling as optimization in the space of measures:\nThe Langevin dynamics as a composite optimization problem</a></li>\n<li><a href=\"https://arxiv.org/pdf/1811.08413.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Sampling Can Be Faster Than Optimization</a></li>\n<li><a href=\"https://arxiv.org/pdf/1704.07520.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Stein Variational Gradient Descent as Gradient Flow</a></li>\n<li><a href=\"https://arxiv.org/pdf/2006.02509.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SVGD as a kernelized Wasserstein gradient flow of the chi-squared divergence</a></li>\n<li><a href=\"https://arxiv.org/pdf/1906.04370.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Maximum Mean Discrepancy Gradient Flow</a></li>\n<li><a href=\"https://arxiv.org/pdf/2006.09797.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">A Non-Asymptotic Analysis for\nStein Variational Gradient Descent</a></li>\n<li><a href=\"https://arxiv.org/pdf/1809.01293.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Stochastic Particle-Optimization Sampling and the Non-Asymptotic Convergence Theory</a></li>\n</ul>\n</li>\n<li>\n<p>Langevin Monte Carlo:</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1412.7392.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Theoretical guarantees for approximate sampling from smooth and log-concave densities</a></li>\n<li><a href=\"https://arxiv.org/pdf/1507.05021.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Non-asymptotic convergence analysis for the Unadjusted Langevin Algorithm</a></li>\n<li><a href=\"https://arxiv.org/pdf/1605.01559.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">High-dimensional Bayesian inference via the Unadjusted\nLangevin Algorithm</a></li>\n<li><a href=\"https://arxiv.org/pdf/1802.09188.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Analysis of Langevin Monte Carlo via convex optimization</a></li>\n<li><a href=\"https://arxiv.org/abs/1710.00095\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient</a></li>\n</ul>\n</li>\n<li>\n<p>Optimization:</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1503.01243.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">A Differential Equation for Modeling Nesterov’s Accelerated\nGradient Method: Theory and Insights</a></li>\n<li><a href=\"https://link.springer.com/book/10.1007%2F978-3-030-39568-1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">First-order and Stochastic Optimization Methods for Machine Learning</a></li>\n<li><a href=\"https://www.prateekjain.org/publications/all_papers/JainK17_FTML.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Non-convex Optimization for Machine Learning</a></li>\n</ul>\n</li>\n</ol>\n<p>Here is <a href=\"https://arxiv.org/pdf/1609.03890.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">an overview of gradient flows by Filippo Santambrogio</a>, <a href=\"https://www.springer.com/gp/book/9781402075537\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">introductory lectures on  convex optimization by Yurii Nesterov</a> and the more exhaustive <a href=\"https://www.springer.com/gp/book/9783319915777\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">lectures on  convex optimization by Yurii Nesterov</a>.</p>\n<p>The webpage and resources for Fall 2019 can be found <a href=\"https://sidvishwanath.com/posts/stag-2019\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">here</a>.</p>\n<hr>\n<h2 id=\"schedule\" style=\"position:relative;\"><a href=\"#schedule\" aria-label=\"schedule permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Schedule</h2>\n<p>The schedule is available on the <a href=\"https://calendar.google.com/calendar?cid=dDNqbXA3MWcyZ2Uya241NGtoN2FmbDM1dWdAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">STAG Google Calendar</a></p>\n<div class=\"gatsby-resp-iframe-wrapper\" style=\"padding-bottom: 50%; position: relative; height: 0; overflow: hidden; margin-bottom: 1.0725rem\" > <iframe src=\"https://calendar.google.com/calendar/embed?height=400&amp;wkst=1&amp;bgcolor=%237CB342&amp;ctz=America%2FNew_York&amp;src=dDNqbXA3MWcyZ2Uya241NGtoN2FmbDM1dWdAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;color=%238E24AA&amp;showTitle=0&amp;showNav=0&amp;showDate=1&amp;showPrint=0&amp;showTabs=1&amp;showCalendars=0&amp;mode=AGENDA\" style=\"border-width:0; position: absolute; top: 0; left: 0; width: 100%; height: 100%; \" frameborder=\"0\" scrolling=\"no\"></iframe> </div>\n<hr>\n<p>If you’re interested in attending the meetings, please <a href=\"https://forms.gle/xFZGmoPqh75gaj4X6\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">sign-up here</a>, and send an email to the <a href=\"mailto:l-stat-stag-subscribe-request@lists.psu.edu\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">L-STAT-STAG</a> with the subject “Add Me” and include your name and department in the body.</p>"}